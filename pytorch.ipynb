{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHoJ9KqTE0oWkO5F+pBR92",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushal2022/Pytorch/blob/main/pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic PyTorch"
      ],
      "metadata": {
        "id": "1Pf1ejwhu_97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "weights = torch.tensor([0.2126, 0.7152, 0.0722], requires_grad=True)\n",
        "\n",
        "for epoch in range(10):\n",
        "  model_output = (weights * 3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "\n",
        "  weights.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh2XfwatvDFK",
        "outputId": "2b2bd291-8496-41f3-b2d4-2ff416676de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "source": [
        "weights = torch.tensor([0.2126, 0.7152, 0.0722], requires_grad=True)\n",
        "\n",
        "# Wrap the weights tensor in a list to make it an iterable\n",
        "optimizer = torch.optim.SGD([weights], lr=0.01)\n",
        "\n",
        "for epoch in range(10):\n",
        "  model_output = (weights * 3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LayKPUUowYCf",
        "outputId": "f5aa45cf-3741-405c-df7d-d6dacdc7ee9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n",
            "tensor([3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back propagation example with pytorch**"
      ],
      "metadata": {
        "id": "-ZPaZbKD0mri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# forward pass and compute the loss\n",
        "y_hat = w * x\n",
        "loss = (y_hat - y)**2\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "### update weights\n",
        "### next forward and backward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGBg1bCD0zKC",
        "outputId": "d45e115b-38db-4ef2-adb3-ea1ded31c3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Prediction : Pytorch Model\n",
        "* Gradients Computation : Autograd\n",
        "* Loss Computation : Pytorch Loss\n",
        "* Parameter Updates : Pytorch Optimizer"
      ],
      "metadata": {
        "id": "2Pr-gwpqtyRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Linear function\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# Model Prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred - y)**2).mean()\n",
        "\n",
        "# Gradient\n",
        "# MSE = 1/N * (w * x - y)**2\n",
        "# dJ/dw = 1/N 2*(w * x - y)\n",
        "def gradient(x, y, y_pred):\n",
        "  return np.dot(2*x, y_pred - y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  # update wieghts\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoch % 2 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "379banWRuFt0",
        "outputId": "73d0c9ba-0769-45fc-c53f-0c16be52b09a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 7: w = 1.997, loss = 0.00050332\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 11: w = 2.000, loss = 0.00000033\n",
            "epoch 13: w = 2.000, loss = 0.00000001\n",
            "epoch 15: w = 2.000, loss = 0.00000000\n",
            "epoch 17: w = 2.000, loss = 0.00000000\n",
            "epoch 19: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Linear function\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# Model Prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Loss = MSE\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred - y)**2).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = backward pass\n",
        "  l.backward() # dl/dw\n",
        "\n",
        "  # update wieghts\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # zero gradients\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoch % 2 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFpdDSKbwXVr",
        "outputId": "d1903862-ca92-45ad-a84b-fc79e2c0e841"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 3: w = 0.772, loss = 15.66018772\n",
            "epoch 5: w = 1.113, loss = 8.17471695\n",
            "epoch 7: w = 1.359, loss = 4.26725292\n",
            "epoch 9: w = 1.537, loss = 2.22753215\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 13: w = 1.758, loss = 0.60698116\n",
            "epoch 15: w = 1.825, loss = 0.31684780\n",
            "epoch 17: w = 1.874, loss = 0.16539653\n",
            "epoch 19: w = 1.909, loss = 0.08633806\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 23: w = 1.952, loss = 0.02352631\n",
            "epoch 25: w = 1.966, loss = 0.01228084\n",
            "epoch 27: w = 1.975, loss = 0.00641066\n",
            "epoch 29: w = 1.982, loss = 0.00334642\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 33: w = 1.991, loss = 0.00091188\n",
            "epoch 35: w = 1.993, loss = 0.00047601\n",
            "epoch 37: w = 1.995, loss = 0.00024848\n",
            "epoch 39: w = 1.996, loss = 0.00012971\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 43: w = 1.998, loss = 0.00003534\n",
            "epoch 45: w = 1.999, loss = 0.00001845\n",
            "epoch 47: w = 1.999, loss = 0.00000963\n",
            "epoch 49: w = 1.999, loss = 0.00000503\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 53: w = 2.000, loss = 0.00000137\n",
            "epoch 55: w = 2.000, loss = 0.00000071\n",
            "epoch 57: w = 2.000, loss = 0.00000037\n",
            "epoch 59: w = 2.000, loss = 0.00000019\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 63: w = 2.000, loss = 0.00000005\n",
            "epoch 65: w = 2.000, loss = 0.00000003\n",
            "epoch 67: w = 2.000, loss = 0.00000001\n",
            "epoch 69: w = 2.000, loss = 0.00000001\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 73: w = 2.000, loss = 0.00000000\n",
            "epoch 75: w = 2.000, loss = 0.00000000\n",
            "epoch 77: w = 2.000, loss = 0.00000000\n",
            "epoch 79: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 83: w = 2.000, loss = 0.00000000\n",
            "epoch 85: w = 2.000, loss = 0.00000000\n",
            "epoch 87: w = 2.000, loss = 0.00000000\n",
            "epoch 89: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "epoch 93: w = 2.000, loss = 0.00000000\n",
            "epoch 95: w = 2.000, loss = 0.00000000\n",
            "epoch 97: w = 2.000, loss = 0.00000000\n",
            "epoch 99: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Design model (input size, output size, forward pass)\n",
        "2. Construct the loss and optimizer\n",
        "3. Training loop\n",
        "  - forward pass : compute prediction\n",
        "  - backward pass : gradients\n",
        "  - update the weights"
      ],
      "metadata": {
        "id": "2IOWBTSmAkuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Linear function\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "#model = nn.Linear(1, 1)\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 1000\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients = backward pass\n",
        "  l.backward() # dl/dw\n",
        "\n",
        "  # update wieghts\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 2 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNo6mmWW_0P1",
        "outputId": "9556295e-e6ea-4e98-a1ba-b1187ed3dafd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 4.745\n",
            "epoch 1: w = 0.953, loss = 6.95962811\n",
            "epoch 3: w = 1.172, loss = 3.43393278\n",
            "epoch 5: w = 1.325, loss = 1.73543131\n",
            "epoch 7: w = 1.432, loss = 0.91667974\n",
            "epoch 9: w = 1.507, loss = 0.52150995\n",
            "epoch 11: w = 1.559, loss = 0.33029252\n",
            "epoch 13: w = 1.596, loss = 0.23728345\n",
            "epoch 15: w = 1.623, loss = 0.19156928\n",
            "epoch 17: w = 1.641, loss = 0.16863762\n",
            "epoch 19: w = 1.655, loss = 0.15668596\n",
            "epoch 21: w = 1.665, loss = 0.15003157\n",
            "epoch 23: w = 1.673, loss = 0.14593840\n",
            "epoch 25: w = 1.678, loss = 0.14308898\n",
            "epoch 27: w = 1.683, loss = 0.14084889\n",
            "epoch 29: w = 1.687, loss = 0.13891256\n",
            "epoch 31: w = 1.690, loss = 0.13713251\n",
            "epoch 33: w = 1.693, loss = 0.13543811\n",
            "epoch 35: w = 1.695, loss = 0.13379464\n",
            "epoch 37: w = 1.697, loss = 0.13218576\n",
            "epoch 39: w = 1.699, loss = 0.13060321\n",
            "epoch 41: w = 1.701, loss = 0.12904294\n",
            "epoch 43: w = 1.703, loss = 0.12750295\n",
            "epoch 45: w = 1.705, loss = 0.12598217\n",
            "epoch 47: w = 1.707, loss = 0.12447985\n",
            "epoch 49: w = 1.709, loss = 0.12299558\n",
            "epoch 51: w = 1.711, loss = 0.12152925\n",
            "epoch 53: w = 1.712, loss = 0.12008031\n",
            "epoch 55: w = 1.714, loss = 0.11864874\n",
            "epoch 57: w = 1.716, loss = 0.11723416\n",
            "epoch 59: w = 1.718, loss = 0.11583649\n",
            "epoch 61: w = 1.719, loss = 0.11445549\n",
            "epoch 63: w = 1.721, loss = 0.11309091\n",
            "epoch 65: w = 1.723, loss = 0.11174273\n",
            "epoch 67: w = 1.724, loss = 0.11041053\n",
            "epoch 69: w = 1.726, loss = 0.10909420\n",
            "epoch 71: w = 1.728, loss = 0.10779360\n",
            "epoch 73: w = 1.729, loss = 0.10650850\n",
            "epoch 75: w = 1.731, loss = 0.10523868\n",
            "epoch 77: w = 1.732, loss = 0.10398401\n",
            "epoch 79: w = 1.734, loss = 0.10274432\n",
            "epoch 81: w = 1.736, loss = 0.10151940\n",
            "epoch 83: w = 1.737, loss = 0.10030916\n",
            "epoch 85: w = 1.739, loss = 0.09911318\n",
            "epoch 87: w = 1.740, loss = 0.09793156\n",
            "epoch 89: w = 1.742, loss = 0.09676403\n",
            "epoch 91: w = 1.743, loss = 0.09561046\n",
            "epoch 93: w = 1.745, loss = 0.09447064\n",
            "epoch 95: w = 1.746, loss = 0.09334435\n",
            "epoch 97: w = 1.748, loss = 0.09223147\n",
            "epoch 99: w = 1.750, loss = 0.09113187\n",
            "epoch 101: w = 1.751, loss = 0.09004541\n",
            "epoch 103: w = 1.752, loss = 0.08897197\n",
            "epoch 105: w = 1.754, loss = 0.08791122\n",
            "epoch 107: w = 1.755, loss = 0.08686313\n",
            "epoch 109: w = 1.757, loss = 0.08582756\n",
            "epoch 111: w = 1.758, loss = 0.08480433\n",
            "epoch 113: w = 1.760, loss = 0.08379326\n",
            "epoch 115: w = 1.761, loss = 0.08279432\n",
            "epoch 117: w = 1.763, loss = 0.08180723\n",
            "epoch 119: w = 1.764, loss = 0.08083192\n",
            "epoch 121: w = 1.765, loss = 0.07986823\n",
            "epoch 123: w = 1.767, loss = 0.07891610\n",
            "epoch 125: w = 1.768, loss = 0.07797531\n",
            "epoch 127: w = 1.770, loss = 0.07704563\n",
            "epoch 129: w = 1.771, loss = 0.07612713\n",
            "epoch 131: w = 1.772, loss = 0.07521951\n",
            "epoch 133: w = 1.774, loss = 0.07432272\n",
            "epoch 135: w = 1.775, loss = 0.07343669\n",
            "epoch 137: w = 1.776, loss = 0.07256117\n",
            "epoch 139: w = 1.778, loss = 0.07169617\n",
            "epoch 141: w = 1.779, loss = 0.07084139\n",
            "epoch 143: w = 1.780, loss = 0.06999683\n",
            "epoch 145: w = 1.782, loss = 0.06916232\n",
            "epoch 147: w = 1.783, loss = 0.06833774\n",
            "epoch 149: w = 1.784, loss = 0.06752308\n",
            "epoch 151: w = 1.786, loss = 0.06671803\n",
            "epoch 153: w = 1.787, loss = 0.06592261\n",
            "epoch 155: w = 1.788, loss = 0.06513674\n",
            "epoch 157: w = 1.789, loss = 0.06436011\n",
            "epoch 159: w = 1.791, loss = 0.06359284\n",
            "epoch 161: w = 1.792, loss = 0.06283469\n",
            "epoch 163: w = 1.793, loss = 0.06208561\n",
            "epoch 165: w = 1.794, loss = 0.06134542\n",
            "epoch 167: w = 1.796, loss = 0.06061408\n",
            "epoch 169: w = 1.797, loss = 0.05989136\n",
            "epoch 171: w = 1.798, loss = 0.05917739\n",
            "epoch 173: w = 1.799, loss = 0.05847190\n",
            "epoch 175: w = 1.801, loss = 0.05777485\n",
            "epoch 177: w = 1.802, loss = 0.05708603\n",
            "epoch 179: w = 1.803, loss = 0.05640546\n",
            "epoch 181: w = 1.804, loss = 0.05573297\n",
            "epoch 183: w = 1.805, loss = 0.05506850\n",
            "epoch 185: w = 1.806, loss = 0.05441201\n",
            "epoch 187: w = 1.808, loss = 0.05376330\n",
            "epoch 189: w = 1.809, loss = 0.05312238\n",
            "epoch 191: w = 1.810, loss = 0.05248906\n",
            "epoch 193: w = 1.811, loss = 0.05186326\n",
            "epoch 195: w = 1.812, loss = 0.05124496\n",
            "epoch 197: w = 1.813, loss = 0.05063401\n",
            "epoch 199: w = 1.814, loss = 0.05003035\n",
            "epoch 201: w = 1.816, loss = 0.04943387\n",
            "epoch 203: w = 1.817, loss = 0.04884462\n",
            "epoch 205: w = 1.818, loss = 0.04826226\n",
            "epoch 207: w = 1.819, loss = 0.04768687\n",
            "epoch 209: w = 1.820, loss = 0.04711834\n",
            "epoch 211: w = 1.821, loss = 0.04655659\n",
            "epoch 213: w = 1.822, loss = 0.04600156\n",
            "epoch 215: w = 1.823, loss = 0.04545308\n",
            "epoch 217: w = 1.824, loss = 0.04491120\n",
            "epoch 219: w = 1.825, loss = 0.04437579\n",
            "epoch 221: w = 1.826, loss = 0.04384672\n",
            "epoch 223: w = 1.827, loss = 0.04332402\n",
            "epoch 225: w = 1.828, loss = 0.04280747\n",
            "epoch 227: w = 1.829, loss = 0.04229712\n",
            "epoch 229: w = 1.830, loss = 0.04179286\n",
            "epoch 231: w = 1.831, loss = 0.04129464\n",
            "epoch 233: w = 1.832, loss = 0.04080229\n",
            "epoch 235: w = 1.833, loss = 0.04031589\n",
            "epoch 237: w = 1.834, loss = 0.03983521\n",
            "epoch 239: w = 1.835, loss = 0.03936034\n",
            "epoch 241: w = 1.836, loss = 0.03889105\n",
            "epoch 243: w = 1.837, loss = 0.03842740\n",
            "epoch 245: w = 1.838, loss = 0.03796927\n",
            "epoch 247: w = 1.839, loss = 0.03751659\n",
            "epoch 249: w = 1.840, loss = 0.03706938\n",
            "epoch 251: w = 1.841, loss = 0.03662738\n",
            "epoch 253: w = 1.842, loss = 0.03619078\n",
            "epoch 255: w = 1.843, loss = 0.03575929\n",
            "epoch 257: w = 1.844, loss = 0.03533291\n",
            "epoch 259: w = 1.845, loss = 0.03491173\n",
            "epoch 261: w = 1.846, loss = 0.03449549\n",
            "epoch 263: w = 1.847, loss = 0.03408427\n",
            "epoch 265: w = 1.848, loss = 0.03367787\n",
            "epoch 267: w = 1.849, loss = 0.03327641\n",
            "epoch 269: w = 1.850, loss = 0.03287967\n",
            "epoch 271: w = 1.850, loss = 0.03248768\n",
            "epoch 273: w = 1.851, loss = 0.03210040\n",
            "epoch 275: w = 1.852, loss = 0.03171767\n",
            "epoch 277: w = 1.853, loss = 0.03133954\n",
            "epoch 279: w = 1.854, loss = 0.03096592\n",
            "epoch 281: w = 1.855, loss = 0.03059675\n",
            "epoch 283: w = 1.856, loss = 0.03023196\n",
            "epoch 285: w = 1.857, loss = 0.02987157\n",
            "epoch 287: w = 1.857, loss = 0.02951543\n",
            "epoch 289: w = 1.858, loss = 0.02916355\n",
            "epoch 291: w = 1.859, loss = 0.02881586\n",
            "epoch 293: w = 1.860, loss = 0.02847231\n",
            "epoch 295: w = 1.861, loss = 0.02813287\n",
            "epoch 297: w = 1.862, loss = 0.02779748\n",
            "epoch 299: w = 1.862, loss = 0.02746610\n",
            "epoch 301: w = 1.863, loss = 0.02713864\n",
            "epoch 303: w = 1.864, loss = 0.02681511\n",
            "epoch 305: w = 1.865, loss = 0.02649537\n",
            "epoch 307: w = 1.866, loss = 0.02617950\n",
            "epoch 309: w = 1.867, loss = 0.02586740\n",
            "epoch 311: w = 1.867, loss = 0.02555905\n",
            "epoch 313: w = 1.868, loss = 0.02525431\n",
            "epoch 315: w = 1.869, loss = 0.02495324\n",
            "epoch 317: w = 1.870, loss = 0.02465572\n",
            "epoch 319: w = 1.870, loss = 0.02436179\n",
            "epoch 321: w = 1.871, loss = 0.02407137\n",
            "epoch 323: w = 1.872, loss = 0.02378434\n",
            "epoch 325: w = 1.873, loss = 0.02350083\n",
            "epoch 327: w = 1.874, loss = 0.02322062\n",
            "epoch 329: w = 1.874, loss = 0.02294377\n",
            "epoch 331: w = 1.875, loss = 0.02267029\n",
            "epoch 333: w = 1.876, loss = 0.02239999\n",
            "epoch 335: w = 1.877, loss = 0.02213293\n",
            "epoch 337: w = 1.877, loss = 0.02186907\n",
            "epoch 339: w = 1.878, loss = 0.02160833\n",
            "epoch 341: w = 1.879, loss = 0.02135072\n",
            "epoch 343: w = 1.879, loss = 0.02109618\n",
            "epoch 345: w = 1.880, loss = 0.02084469\n",
            "epoch 347: w = 1.881, loss = 0.02059619\n",
            "epoch 349: w = 1.882, loss = 0.02035062\n",
            "epoch 351: w = 1.882, loss = 0.02010800\n",
            "epoch 353: w = 1.883, loss = 0.01986825\n",
            "epoch 355: w = 1.884, loss = 0.01963140\n",
            "epoch 357: w = 1.884, loss = 0.01939737\n",
            "epoch 359: w = 1.885, loss = 0.01916612\n",
            "epoch 361: w = 1.886, loss = 0.01893760\n",
            "epoch 363: w = 1.886, loss = 0.01871184\n",
            "epoch 365: w = 1.887, loss = 0.01848875\n",
            "epoch 367: w = 1.888, loss = 0.01826832\n",
            "epoch 369: w = 1.889, loss = 0.01805052\n",
            "epoch 371: w = 1.889, loss = 0.01783533\n",
            "epoch 373: w = 1.890, loss = 0.01762272\n",
            "epoch 375: w = 1.891, loss = 0.01741260\n",
            "epoch 377: w = 1.891, loss = 0.01720501\n",
            "epoch 379: w = 1.892, loss = 0.01699992\n",
            "epoch 381: w = 1.892, loss = 0.01679721\n",
            "epoch 383: w = 1.893, loss = 0.01659695\n",
            "epoch 385: w = 1.894, loss = 0.01639909\n",
            "epoch 387: w = 1.894, loss = 0.01620357\n",
            "epoch 389: w = 1.895, loss = 0.01601042\n",
            "epoch 391: w = 1.896, loss = 0.01581955\n",
            "epoch 393: w = 1.896, loss = 0.01563095\n",
            "epoch 395: w = 1.897, loss = 0.01544460\n",
            "epoch 397: w = 1.897, loss = 0.01526047\n",
            "epoch 399: w = 1.898, loss = 0.01507854\n",
            "epoch 401: w = 1.899, loss = 0.01489875\n",
            "epoch 403: w = 1.899, loss = 0.01472112\n",
            "epoch 405: w = 1.900, loss = 0.01454563\n",
            "epoch 407: w = 1.901, loss = 0.01437220\n",
            "epoch 409: w = 1.901, loss = 0.01420085\n",
            "epoch 411: w = 1.902, loss = 0.01403156\n",
            "epoch 413: w = 1.902, loss = 0.01386426\n",
            "epoch 415: w = 1.903, loss = 0.01369900\n",
            "epoch 417: w = 1.903, loss = 0.01353569\n",
            "epoch 419: w = 1.904, loss = 0.01337429\n",
            "epoch 421: w = 1.905, loss = 0.01321486\n",
            "epoch 423: w = 1.905, loss = 0.01305731\n",
            "epoch 425: w = 1.906, loss = 0.01290166\n",
            "epoch 427: w = 1.906, loss = 0.01274782\n",
            "epoch 429: w = 1.907, loss = 0.01259585\n",
            "epoch 431: w = 1.907, loss = 0.01244570\n",
            "epoch 433: w = 1.908, loss = 0.01229731\n",
            "epoch 435: w = 1.909, loss = 0.01215072\n",
            "epoch 437: w = 1.909, loss = 0.01200584\n",
            "epoch 439: w = 1.910, loss = 0.01186272\n",
            "epoch 441: w = 1.910, loss = 0.01172127\n",
            "epoch 443: w = 1.911, loss = 0.01158152\n",
            "epoch 445: w = 1.911, loss = 0.01144348\n",
            "epoch 447: w = 1.912, loss = 0.01130704\n",
            "epoch 449: w = 1.912, loss = 0.01117223\n",
            "epoch 451: w = 1.913, loss = 0.01103904\n",
            "epoch 453: w = 1.913, loss = 0.01090745\n",
            "epoch 455: w = 1.914, loss = 0.01077739\n",
            "epoch 457: w = 1.914, loss = 0.01064891\n",
            "epoch 459: w = 1.915, loss = 0.01052196\n",
            "epoch 461: w = 1.915, loss = 0.01039652\n",
            "epoch 463: w = 1.916, loss = 0.01027255\n",
            "epoch 465: w = 1.916, loss = 0.01015011\n",
            "epoch 467: w = 1.917, loss = 0.01002910\n",
            "epoch 469: w = 1.917, loss = 0.00990952\n",
            "epoch 471: w = 1.918, loss = 0.00979138\n",
            "epoch 473: w = 1.918, loss = 0.00967465\n",
            "epoch 475: w = 1.919, loss = 0.00955930\n",
            "epoch 477: w = 1.919, loss = 0.00944533\n",
            "epoch 479: w = 1.920, loss = 0.00933275\n",
            "epoch 481: w = 1.920, loss = 0.00922149\n",
            "epoch 483: w = 1.921, loss = 0.00911153\n",
            "epoch 485: w = 1.921, loss = 0.00900292\n",
            "epoch 487: w = 1.922, loss = 0.00889560\n",
            "epoch 489: w = 1.922, loss = 0.00878953\n",
            "epoch 491: w = 1.923, loss = 0.00868474\n",
            "epoch 493: w = 1.923, loss = 0.00858121\n",
            "epoch 495: w = 1.924, loss = 0.00847889\n",
            "epoch 497: w = 1.924, loss = 0.00837781\n",
            "epoch 499: w = 1.925, loss = 0.00827794\n",
            "epoch 501: w = 1.925, loss = 0.00817923\n",
            "epoch 503: w = 1.925, loss = 0.00808173\n",
            "epoch 505: w = 1.926, loss = 0.00798539\n",
            "epoch 507: w = 1.926, loss = 0.00789018\n",
            "epoch 509: w = 1.927, loss = 0.00779610\n",
            "epoch 511: w = 1.927, loss = 0.00770316\n",
            "epoch 513: w = 1.928, loss = 0.00761132\n",
            "epoch 515: w = 1.928, loss = 0.00752060\n",
            "epoch 517: w = 1.928, loss = 0.00743093\n",
            "epoch 519: w = 1.929, loss = 0.00734233\n",
            "epoch 521: w = 1.929, loss = 0.00725480\n",
            "epoch 523: w = 1.930, loss = 0.00716832\n",
            "epoch 525: w = 1.930, loss = 0.00708285\n",
            "epoch 527: w = 1.931, loss = 0.00699841\n",
            "epoch 529: w = 1.931, loss = 0.00691498\n",
            "epoch 531: w = 1.931, loss = 0.00683253\n",
            "epoch 533: w = 1.932, loss = 0.00675108\n",
            "epoch 535: w = 1.932, loss = 0.00667059\n",
            "epoch 537: w = 1.933, loss = 0.00659106\n",
            "epoch 539: w = 1.933, loss = 0.00651248\n",
            "epoch 541: w = 1.933, loss = 0.00643485\n",
            "epoch 543: w = 1.934, loss = 0.00635812\n",
            "epoch 545: w = 1.934, loss = 0.00628234\n",
            "epoch 547: w = 1.935, loss = 0.00620743\n",
            "epoch 549: w = 1.935, loss = 0.00613344\n",
            "epoch 551: w = 1.935, loss = 0.00606029\n",
            "epoch 553: w = 1.936, loss = 0.00598804\n",
            "epoch 555: w = 1.936, loss = 0.00591667\n",
            "epoch 557: w = 1.937, loss = 0.00584613\n",
            "epoch 559: w = 1.937, loss = 0.00577642\n",
            "epoch 561: w = 1.937, loss = 0.00570755\n",
            "epoch 563: w = 1.938, loss = 0.00563952\n",
            "epoch 565: w = 1.938, loss = 0.00557227\n",
            "epoch 567: w = 1.938, loss = 0.00550585\n",
            "epoch 569: w = 1.939, loss = 0.00544021\n",
            "epoch 571: w = 1.939, loss = 0.00537534\n",
            "epoch 573: w = 1.940, loss = 0.00531127\n",
            "epoch 575: w = 1.940, loss = 0.00524795\n",
            "epoch 577: w = 1.940, loss = 0.00518537\n",
            "epoch 579: w = 1.941, loss = 0.00512357\n",
            "epoch 581: w = 1.941, loss = 0.00506248\n",
            "epoch 583: w = 1.941, loss = 0.00500213\n",
            "epoch 585: w = 1.942, loss = 0.00494249\n",
            "epoch 587: w = 1.942, loss = 0.00488357\n",
            "epoch 589: w = 1.942, loss = 0.00482535\n",
            "epoch 591: w = 1.943, loss = 0.00476782\n",
            "epoch 593: w = 1.943, loss = 0.00471097\n",
            "epoch 595: w = 1.943, loss = 0.00465480\n",
            "epoch 597: w = 1.944, loss = 0.00459932\n",
            "epoch 599: w = 1.944, loss = 0.00454448\n",
            "epoch 601: w = 1.944, loss = 0.00449031\n",
            "epoch 603: w = 1.945, loss = 0.00443677\n",
            "epoch 605: w = 1.945, loss = 0.00438387\n",
            "epoch 607: w = 1.945, loss = 0.00433162\n",
            "epoch 609: w = 1.946, loss = 0.00427998\n",
            "epoch 611: w = 1.946, loss = 0.00422894\n",
            "epoch 613: w = 1.946, loss = 0.00417852\n",
            "epoch 615: w = 1.947, loss = 0.00412872\n",
            "epoch 617: w = 1.947, loss = 0.00407950\n",
            "epoch 619: w = 1.947, loss = 0.00403086\n",
            "epoch 621: w = 1.948, loss = 0.00398280\n",
            "epoch 623: w = 1.948, loss = 0.00393533\n",
            "epoch 625: w = 1.948, loss = 0.00388840\n",
            "epoch 627: w = 1.949, loss = 0.00384205\n",
            "epoch 629: w = 1.949, loss = 0.00379624\n",
            "epoch 631: w = 1.949, loss = 0.00375099\n",
            "epoch 633: w = 1.949, loss = 0.00370627\n",
            "epoch 635: w = 1.950, loss = 0.00366208\n",
            "epoch 637: w = 1.950, loss = 0.00361841\n",
            "epoch 639: w = 1.950, loss = 0.00357528\n",
            "epoch 641: w = 1.951, loss = 0.00353265\n",
            "epoch 643: w = 1.951, loss = 0.00349053\n",
            "epoch 645: w = 1.951, loss = 0.00344893\n",
            "epoch 647: w = 1.952, loss = 0.00340781\n",
            "epoch 649: w = 1.952, loss = 0.00336718\n",
            "epoch 651: w = 1.952, loss = 0.00332702\n",
            "epoch 653: w = 1.952, loss = 0.00328737\n",
            "epoch 655: w = 1.953, loss = 0.00324817\n",
            "epoch 657: w = 1.953, loss = 0.00320945\n",
            "epoch 659: w = 1.953, loss = 0.00317119\n",
            "epoch 661: w = 1.954, loss = 0.00313337\n",
            "epoch 663: w = 1.954, loss = 0.00309602\n",
            "epoch 665: w = 1.954, loss = 0.00305911\n",
            "epoch 667: w = 1.954, loss = 0.00302264\n",
            "epoch 669: w = 1.955, loss = 0.00298662\n",
            "epoch 671: w = 1.955, loss = 0.00295100\n",
            "epoch 673: w = 1.955, loss = 0.00291581\n",
            "epoch 675: w = 1.955, loss = 0.00288105\n",
            "epoch 677: w = 1.956, loss = 0.00284671\n",
            "epoch 679: w = 1.956, loss = 0.00281278\n",
            "epoch 681: w = 1.956, loss = 0.00277923\n",
            "epoch 683: w = 1.957, loss = 0.00274610\n",
            "epoch 685: w = 1.957, loss = 0.00271336\n",
            "epoch 687: w = 1.957, loss = 0.00268102\n",
            "epoch 689: w = 1.957, loss = 0.00264905\n",
            "epoch 691: w = 1.958, loss = 0.00261747\n",
            "epoch 693: w = 1.958, loss = 0.00258627\n",
            "epoch 695: w = 1.958, loss = 0.00255543\n",
            "epoch 697: w = 1.958, loss = 0.00252496\n",
            "epoch 699: w = 1.959, loss = 0.00249486\n",
            "epoch 701: w = 1.959, loss = 0.00246512\n",
            "epoch 703: w = 1.959, loss = 0.00243573\n",
            "epoch 705: w = 1.959, loss = 0.00240669\n",
            "epoch 707: w = 1.960, loss = 0.00237800\n",
            "epoch 709: w = 1.960, loss = 0.00234965\n",
            "epoch 711: w = 1.960, loss = 0.00232164\n",
            "epoch 713: w = 1.960, loss = 0.00229396\n",
            "epoch 715: w = 1.960, loss = 0.00226661\n",
            "epoch 717: w = 1.961, loss = 0.00223959\n",
            "epoch 719: w = 1.961, loss = 0.00221289\n",
            "epoch 721: w = 1.961, loss = 0.00218651\n",
            "epoch 723: w = 1.961, loss = 0.00216045\n",
            "epoch 725: w = 1.962, loss = 0.00213469\n",
            "epoch 727: w = 1.962, loss = 0.00210923\n",
            "epoch 729: w = 1.962, loss = 0.00208409\n",
            "epoch 731: w = 1.962, loss = 0.00205925\n",
            "epoch 733: w = 1.963, loss = 0.00203469\n",
            "epoch 735: w = 1.963, loss = 0.00201043\n",
            "epoch 737: w = 1.963, loss = 0.00198647\n",
            "epoch 739: w = 1.963, loss = 0.00196279\n",
            "epoch 741: w = 1.963, loss = 0.00193939\n",
            "epoch 743: w = 1.964, loss = 0.00191627\n",
            "epoch 745: w = 1.964, loss = 0.00189342\n",
            "epoch 747: w = 1.964, loss = 0.00187085\n",
            "epoch 749: w = 1.964, loss = 0.00184854\n",
            "epoch 751: w = 1.965, loss = 0.00182651\n",
            "epoch 753: w = 1.965, loss = 0.00180473\n",
            "epoch 755: w = 1.965, loss = 0.00178322\n",
            "epoch 757: w = 1.965, loss = 0.00176195\n",
            "epoch 759: w = 1.965, loss = 0.00174094\n",
            "epoch 761: w = 1.966, loss = 0.00172019\n",
            "epoch 763: w = 1.966, loss = 0.00169968\n",
            "epoch 765: w = 1.966, loss = 0.00167943\n",
            "epoch 767: w = 1.966, loss = 0.00165940\n",
            "epoch 769: w = 1.966, loss = 0.00163962\n",
            "epoch 771: w = 1.967, loss = 0.00162007\n",
            "epoch 773: w = 1.967, loss = 0.00160075\n",
            "epoch 775: w = 1.967, loss = 0.00158168\n",
            "epoch 777: w = 1.967, loss = 0.00156282\n",
            "epoch 779: w = 1.967, loss = 0.00154419\n",
            "epoch 781: w = 1.968, loss = 0.00152578\n",
            "epoch 783: w = 1.968, loss = 0.00150758\n",
            "epoch 785: w = 1.968, loss = 0.00148960\n",
            "epoch 787: w = 1.968, loss = 0.00147185\n",
            "epoch 789: w = 1.968, loss = 0.00145430\n",
            "epoch 791: w = 1.969, loss = 0.00143696\n",
            "epoch 793: w = 1.969, loss = 0.00141983\n",
            "epoch 795: w = 1.969, loss = 0.00140291\n",
            "epoch 797: w = 1.969, loss = 0.00138618\n",
            "epoch 799: w = 1.969, loss = 0.00136966\n",
            "epoch 801: w = 1.969, loss = 0.00135333\n",
            "epoch 803: w = 1.970, loss = 0.00133719\n",
            "epoch 805: w = 1.970, loss = 0.00132125\n",
            "epoch 807: w = 1.970, loss = 0.00130550\n",
            "epoch 809: w = 1.970, loss = 0.00128993\n",
            "epoch 811: w = 1.970, loss = 0.00127455\n",
            "epoch 813: w = 1.971, loss = 0.00125936\n",
            "epoch 815: w = 1.971, loss = 0.00124434\n",
            "epoch 817: w = 1.971, loss = 0.00122951\n",
            "epoch 819: w = 1.971, loss = 0.00121485\n",
            "epoch 821: w = 1.971, loss = 0.00120037\n",
            "epoch 823: w = 1.971, loss = 0.00118606\n",
            "epoch 825: w = 1.972, loss = 0.00117192\n",
            "epoch 827: w = 1.972, loss = 0.00115795\n",
            "epoch 829: w = 1.972, loss = 0.00114414\n",
            "epoch 831: w = 1.972, loss = 0.00113050\n",
            "epoch 833: w = 1.972, loss = 0.00111702\n",
            "epoch 835: w = 1.972, loss = 0.00110371\n",
            "epoch 837: w = 1.973, loss = 0.00109055\n",
            "epoch 839: w = 1.973, loss = 0.00107755\n",
            "epoch 841: w = 1.973, loss = 0.00106471\n",
            "epoch 843: w = 1.973, loss = 0.00105200\n",
            "epoch 845: w = 1.973, loss = 0.00103946\n",
            "epoch 847: w = 1.973, loss = 0.00102707\n",
            "epoch 849: w = 1.974, loss = 0.00101483\n",
            "epoch 851: w = 1.974, loss = 0.00100274\n",
            "epoch 853: w = 1.974, loss = 0.00099078\n",
            "epoch 855: w = 1.974, loss = 0.00097896\n",
            "epoch 857: w = 1.974, loss = 0.00096729\n",
            "epoch 859: w = 1.974, loss = 0.00095576\n",
            "epoch 861: w = 1.975, loss = 0.00094437\n",
            "epoch 863: w = 1.975, loss = 0.00093311\n",
            "epoch 865: w = 1.975, loss = 0.00092198\n",
            "epoch 867: w = 1.975, loss = 0.00091099\n",
            "epoch 869: w = 1.975, loss = 0.00090013\n",
            "epoch 871: w = 1.975, loss = 0.00088940\n",
            "epoch 873: w = 1.975, loss = 0.00087880\n",
            "epoch 875: w = 1.976, loss = 0.00086832\n",
            "epoch 877: w = 1.976, loss = 0.00085797\n",
            "epoch 879: w = 1.976, loss = 0.00084774\n",
            "epoch 881: w = 1.976, loss = 0.00083763\n",
            "epoch 883: w = 1.976, loss = 0.00082764\n",
            "epoch 885: w = 1.976, loss = 0.00081778\n",
            "epoch 887: w = 1.976, loss = 0.00080803\n",
            "epoch 889: w = 1.977, loss = 0.00079839\n",
            "epoch 891: w = 1.977, loss = 0.00078888\n",
            "epoch 893: w = 1.977, loss = 0.00077947\n",
            "epoch 895: w = 1.977, loss = 0.00077018\n",
            "epoch 897: w = 1.977, loss = 0.00076100\n",
            "epoch 899: w = 1.977, loss = 0.00075192\n",
            "epoch 901: w = 1.977, loss = 0.00074296\n",
            "epoch 903: w = 1.978, loss = 0.00073411\n",
            "epoch 905: w = 1.978, loss = 0.00072535\n",
            "epoch 907: w = 1.978, loss = 0.00071670\n",
            "epoch 909: w = 1.978, loss = 0.00070816\n",
            "epoch 911: w = 1.978, loss = 0.00069971\n",
            "epoch 913: w = 1.978, loss = 0.00069137\n",
            "epoch 915: w = 1.978, loss = 0.00068313\n",
            "epoch 917: w = 1.978, loss = 0.00067499\n",
            "epoch 919: w = 1.979, loss = 0.00066694\n",
            "epoch 921: w = 1.979, loss = 0.00065898\n",
            "epoch 923: w = 1.979, loss = 0.00065113\n",
            "epoch 925: w = 1.979, loss = 0.00064337\n",
            "epoch 927: w = 1.979, loss = 0.00063570\n",
            "epoch 929: w = 1.979, loss = 0.00062812\n",
            "epoch 931: w = 1.979, loss = 0.00062063\n",
            "epoch 933: w = 1.979, loss = 0.00061323\n",
            "epoch 935: w = 1.980, loss = 0.00060592\n",
            "epoch 937: w = 1.980, loss = 0.00059870\n",
            "epoch 939: w = 1.980, loss = 0.00059156\n",
            "epoch 941: w = 1.980, loss = 0.00058451\n",
            "epoch 943: w = 1.980, loss = 0.00057754\n",
            "epoch 945: w = 1.980, loss = 0.00057066\n",
            "epoch 947: w = 1.980, loss = 0.00056385\n",
            "epoch 949: w = 1.980, loss = 0.00055713\n",
            "epoch 951: w = 1.981, loss = 0.00055048\n",
            "epoch 953: w = 1.981, loss = 0.00054392\n",
            "epoch 955: w = 1.981, loss = 0.00053744\n",
            "epoch 957: w = 1.981, loss = 0.00053103\n",
            "epoch 959: w = 1.981, loss = 0.00052470\n",
            "epoch 961: w = 1.981, loss = 0.00051845\n",
            "epoch 963: w = 1.981, loss = 0.00051226\n",
            "epoch 965: w = 1.981, loss = 0.00050616\n",
            "epoch 967: w = 1.981, loss = 0.00050012\n",
            "epoch 969: w = 1.982, loss = 0.00049416\n",
            "epoch 971: w = 1.982, loss = 0.00048827\n",
            "epoch 973: w = 1.982, loss = 0.00048245\n",
            "epoch 975: w = 1.982, loss = 0.00047670\n",
            "epoch 977: w = 1.982, loss = 0.00047101\n",
            "epoch 979: w = 1.982, loss = 0.00046540\n",
            "epoch 981: w = 1.982, loss = 0.00045985\n",
            "epoch 983: w = 1.982, loss = 0.00045437\n",
            "epoch 985: w = 1.982, loss = 0.00044895\n",
            "epoch 987: w = 1.983, loss = 0.00044360\n",
            "epoch 989: w = 1.983, loss = 0.00043831\n",
            "epoch 991: w = 1.983, loss = 0.00043308\n",
            "epoch 993: w = 1.983, loss = 0.00042792\n",
            "epoch 995: w = 1.983, loss = 0.00042282\n",
            "epoch 997: w = 1.983, loss = 0.00041778\n",
            "epoch 999: w = 1.983, loss = 0.00041280\n",
            "Prediction after training: f(5) = 9.965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMUUGVmLBnXh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}